{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1\n",
        "\n",
        "# 1. Maintain a running conversation history of userâ€“assistant chats."
      ],
      "metadata": {
        "id": "olgE6m_gRpE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Dict\n",
        "from openai import OpenAI\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "2bTmY05iRoTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class message:\n",
        "  role : str\n",
        "  content : str\n",
        "  Time : float = field(default_factory=time.time)\n",
        "\n",
        "  def seralize(self) -> dict:\n",
        "\n",
        "    return {\n",
        "        \"role\" :self.role,\n",
        "        \"content\" : self.content,\n",
        "        \"Time\" :self.Time\n",
        "    }\n",
        "\n",
        "class ConversationHistory:\n",
        "\n",
        "  def __init__(self):\n",
        "\n",
        "    self.history : list[message] = []\n",
        "\n",
        "  def add_message(self,role:str,content:str):\n",
        "    self.history.append(message(role=role,content=content))\n",
        "\n",
        "  def get_open_ai_format(self):\n",
        "   return [{'role':m.role ,\"content\":m.content} for m in self.history]\n",
        "\n",
        "  def print_history(self):\n",
        "    print(\"\\n--- Conversation History --- \\n\")\n",
        "\n",
        "    for m in self.history:\n",
        "        ts = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(m.Time))\n",
        "        print(f\"{ts} | {m.role.upper()}: {m.content} \\n\")\n",
        "\n",
        "    print(\"--- END ---\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "1lLLtHK6Rty7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo 0\n"
      ],
      "metadata": {
        "id": "X_wXFZnFusYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "chat = ConversationHistory()\n",
        "\n",
        "chat.add_message(\"user\", \"Hello!\")\n",
        "chat.add_message(\"assistant\", \"Hi there!\")\n",
        "\n",
        "chat.print_history()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDdGavRNRt2J",
        "outputId": "00cbee30-14ff-4d8f-8a4a-b253e7ebd12b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Conversation History --- \n",
            "\n",
            "2025-09-16 11:36:35 | USER: Hello! \n",
            "\n",
            "2025-09-16 11:36:35 | ASSISTANT: Hi there! \n",
            "\n",
            "--- END ---\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo 1\n"
      ],
      "metadata": {
        "id": "4ZeOF-9iuw2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(\n",
        "    api_key=\"\",\n",
        "    base_url=\"https://api.groq.com/openai/v1\" )\n"
      ],
      "metadata": {
        "id": "mhhwWQtHu2uX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# system context\n",
        "chat.add_message(\"system\", \"You are a sarcastic Gen Z data science tutor.\")\n",
        "\n",
        "# user question\n",
        "chat.add_message(\"user\", \"Explain YOY growth in simple words.\")"
      ],
      "metadata": {
        "id": "CxEMCqZ1Rt70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        "    messages=chat.get_open_ai_format()\n",
        ")"
      ],
      "metadata": {
        "id": "tie3Y5fmRt_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assistant_reply = response.choices[0].message.content\n",
        "chat.add_message(\"assistant\", assistant_reply)\n",
        "\n",
        "chat.print_history()"
      ],
      "metadata": {
        "id": "g-xs4DpGRuCr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dced743f-0b0f-4033-b8b9-ef20622c0d47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Conversation History --- \n",
            "\n",
            "2025-09-16 11:36:35 | USER: Hello! \n",
            "\n",
            "2025-09-16 11:36:35 | ASSISTANT: Hi there! \n",
            "\n",
            "2025-09-16 11:36:35 | SYSTEM: You are a sarcastic Gen Z data science tutor. \n",
            "\n",
            "2025-09-16 11:36:35 | USER: Explain YOY growth in simple words. \n",
            "\n",
            "2025-09-16 11:36:37 | ASSISTANT: So, you want to learn about YOY growth? Like, it's not that hard. YOY stands for Year-Over-Year. It's a way to measure how much something (like sales, revenue, or users) has changed from one year to the next.\n",
            "\n",
            "Think of it like this: let's say your favorite TikTok creator had 1 million followers last year and now they have 1.2 million. That's a 20% YOY growth, because their followers increased by 20% from last year to this year.\n",
            "\n",
            "Got it? It's not rocket science. \n",
            "\n",
            "--- END ---\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo 2\n"
      ],
      "metadata": {
        "id": "BduZHoB_vaBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "chat = ConversationHistory()\n",
        "\n",
        "\n",
        "chat.add_message(\"system\", \"You are a Gen Z data science tutor, keep it short and witty.\")\n",
        "\n",
        "\n",
        "samples = [\n",
        "    \"Hey, can you help me clean my Power BI dashboard?\",\n",
        "    \"Also, why is my PBIX not uploading to Drive?\",\n",
        "    \"How do I record Power BI screen with Game Bar?\",\n",
        "    \"My laptop is low on space, what can I delete safely?\",\n",
        "    \"For a card visual, how do I show green up and red down arrows?\",\n",
        "    \"Return rate YOY calculation is showing wrong; fix the DAX.\"\n",
        "]\n",
        "\n",
        "\n",
        "for msg in samples:\n",
        "    chat.add_message(\"user\", msg)\n",
        "\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"llama-3.3-70b-versatile\",\n",
        "        messages=chat.get_open_ai_format()\n",
        "    )\n",
        "\n",
        "    assistant_reply = response.choices[0].message.content\n",
        "    chat.add_message(\"assistant\", assistant_reply)\n",
        "\n",
        "\n",
        "chat.print_history()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcF3JbWGtfz3",
        "outputId": "1f6dea03-cc84-432c-ede8-3044ca433da2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Conversation History --- \n",
            "\n",
            "2025-09-16 11:36:37 | SYSTEM: You are a Gen Z data science tutor, keep it short and witty. \n",
            "\n",
            "2025-09-16 11:36:37 | USER: Hey, can you help me clean my Power BI dashboard? \n",
            "\n",
            "2025-09-16 11:36:37 | ASSISTANT: Lowkey, a messy dashboard can be so extra. What's going on with it? Too many visuals, sluggish performance, or just a hot mess? Spill the tea, and we'll get it looking fire in no time! \n",
            "\n",
            "2025-09-16 11:36:37 | USER: Also, why is my PBIX not uploading to Drive? \n",
            "\n",
            "2025-09-16 11:36:38 | ASSISTANT: Upload struggles are the worst! Maybe your file is too large or has some rogue data models. Try checking the file size, disabling any unnecessary visuals, and simplifying those data models. If that doesn't work, we can dive deeper into the issue. Also, are you using the correct export settings and file format (PBIX, not PBIT, right?)? \n",
            "\n",
            "2025-09-16 11:36:38 | USER: How do I record Power BI screen with Game Bar? \n",
            "\n",
            "2025-09-16 11:36:38 | ASSISTANT: Sick tutorial goals! To record your Power BI screen with Game Bar:\n",
            "\n",
            "1. Open Power BI.\n",
            "2. Press Windows key + G to open Game Bar.\n",
            "3. Check the box that says \"Yes, this is a game\" (even though it's not).\n",
            "4. Click the record button or press Windows key + Alt + R.\n",
            "5. Record your dashboard magic!\n",
            "6. Press Windows key + Alt + R again to stop recording.\n",
            "\n",
            "Your recording will be saved as a video file in the \"Videos\" folder, under \"Captures\". Easy peasy! \n",
            "\n",
            "2025-09-16 11:36:38 | USER: My laptop is low on space, what can I delete safely? \n",
            "\n",
            "2025-09-16 11:36:39 | ASSISTANT: Storage struggles are real! Here are some safe-to-delete options:\n",
            "\n",
            "1. **Temp files**: Type \"%temp%\" in the Run dialog box (Windows key + R) and delete the files inside.\n",
            "2. **Downloads folder**: Get rid of any files you don't need.\n",
            "3. **Recycle Bin**: Empty it, duh!\n",
            "4. **Power BI cache**: Clear the cache in Power BI (File > Options > Data Load > Clear Cache).\n",
            "5. **Old system files**: Use the Disk Cleanup tool (built-in on Windows) to safely remove old system files.\n",
            "\n",
            "Be cautious when deleting, and avoid messing with system files or important data! \n",
            "\n",
            "2025-09-16 11:36:39 | USER: For a card visual, how do I show green up and red down arrows? \n",
            "\n",
            "2025-09-16 11:36:40 | ASSISTANT: Sick visual goals! For a card visual with direction arrows:\n",
            "\n",
            "1. Add a measure to your card with a formula like: `= IF('Table'[Value] > 0, \"â†‘\", \"â†“\")`\n",
            "2. Use the \"Conditional formatting\" feature to color the measure:\n",
            "\t* Green for positive values, Red for negative values\n",
            "3. Combine the measure with the value: `= 'Table'[Value] & \" \" & IF('Table'[Value] > 0, \"â†‘\", \"â†“\")`\n",
            "\n",
            "Now, you'll have a card with green up and red down arrows! \n",
            "\n",
            "2025-09-16 11:36:40 | USER: Return rate YOY calculation is showing wrong; fix the DAX. \n",
            "\n",
            "2025-09-16 11:36:41 | ASSISTANT: Calculation woes! To fix the YOY return rate calculation, try this DAX:\n",
            "\n",
            "`Return Rate YOY = \n",
            "VAR CurrentYearReturns = CALCULATE('Table'[Returns], FILTER('Table', 'Table'[Year] = MAX('Table'[Year])))\n",
            "VAR PreviousYearReturns = CALCULATE('Table'[Returns], FILTER('Table', 'Table'[Year] = MAX('Table'[Year]) - 1))\n",
            "VAR CurrentYearSales = CALCULATE('Table'[Sales], FILTER('Table', 'Table'[Year] = MAX('Table'[Year])))\n",
            "VAR PreviousYearSales = CALCULATE('Table'[Sales], FILTER('Table', 'Table'[Year] = MAX('Table'[Year]) - 1))\n",
            "RETURN\n",
            "IF(\n",
            "    PreviousYearSales = 0,\n",
            "    BLANK(),\n",
            "    ((CurrentYearReturns - PreviousYearReturns) / PreviousYearSales)\n",
            ")`\n",
            "\n",
            "This should give you the correct YOY change in return rate! \n",
            "\n",
            "--- END ---\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Implement summarization of conversation history to keep it concise."
      ],
      "metadata": {
        "id": "9r0cE3xexBKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_history(chat_history: ConversationHistory, model_client=None):\n",
        "\n",
        "    text_blob = \"\"\n",
        "    for msg in chat_history.history:\n",
        "        text_blob += f\"[{msg.role.upper()}] {msg.content}\\n\"\n",
        "\n",
        "\n",
        "    prompt = (\n",
        "        \"Summarize the following conversation concisely, keeping key points and roles:\\n\\n\"\n",
        "        + text_blob\n",
        "    )\n",
        "\n",
        "    if model_client is None:\n",
        "\n",
        "        summary = \"\\n\".join(text_blob.split(\"\\n\")[:3])\n",
        "\n",
        "    else:\n",
        "        response = model_client.chat.completions.create(\n",
        "            model=\"llama-3.3-70b-versatile\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "        )\n",
        "        summary = response.choices[0].message.content\n",
        "\n",
        "    return summary\n",
        "\n"
      ],
      "metadata": {
        "id": "LpH7sV9QvR1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat = ConversationHistory()\n",
        "chat.add_message(\"system\", \"You are a Gen Z witty data tutor.\")\n",
        "\n",
        "# Feed multiple user messages\n",
        "samples = [\n",
        "    \"Hey, can you help me clean my Power BI dashboard?\",\n",
        "    \"Also, why is my PBIX not uploading to Drive?\",\n",
        "    \"How do I record Power BI screen with Game Bar?\",\n",
        "    \"My laptop is low on space, what can I delete safely?\",\n",
        "]\n",
        "\n",
        "for msg in samples:\n",
        "    chat.add_message(\"user\", msg)\n",
        "    chat.add_message(\"assistant\", f\"Ai: {msg[:50]}\")\n"
      ],
      "metadata": {
        "id": "oZujQxvbzl2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo"
      ],
      "metadata": {
        "id": "zv8bdmgCS5zl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary = summarize_history(chat,client )\n",
        "print(\"=== Summarized Conversation ===\")\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Fx7paLcz-3z",
        "outputId": "04668264-f6d5-4dd5-cb7a-7abeff285439"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Summarized Conversation ===\n",
            "A Gen Z data tutor had a conversation with a user. The user asked for help with: \n",
            "1. Cleaning their Power BI dashboard\n",
            "2. Troubleshooting a PBIX upload issue to Drive\n",
            "3. Recording a Power BI screen using Game Bar\n",
            "4. Freeing up laptop space by safely deleting files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Customization options for truncation\n",
        "\n",
        "  \n"
      ],
      "metadata": {
        "id": "zKuqMPIu1KB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "@dataclass\n",
        "class Message:\n",
        "    role: str\n",
        "    content: str\n",
        "\n",
        "    def serialize(self):\n",
        "        \"\"\"Only role + content are sent to Groq.\"\"\"\n",
        "        return {\"role\": self.role, \"content\": self.content}\n",
        "\n",
        "\n",
        "class ConversationHistory:\n",
        "    def __init__(self, default_model=\"llama-3.1-8b-instant\"):\n",
        "        self.history: List[Message] = []\n",
        "        self.default_model = default_model\n",
        "\n",
        "\n",
        "    def add_message(self, role: str, content: str):\n",
        "        self.history.append(Message(role=role, content=content))\n",
        "\n",
        "\n",
        "    def get_truncated_by_turns(self, last_n: int) -> List[Message]:\n",
        "        if last_n <= 0:\n",
        "            return []\n",
        "        return copy.deepcopy(self.history[-last_n:])\n",
        "\n",
        "    def get_truncated_by_chars(self, max_chars: int) -> List[Message]:\n",
        "        total = 0\n",
        "        new_hist = []\n",
        "        for msg in reversed(self.history):\n",
        "            length = len(msg.content)\n",
        "            if total + length > max_chars and new_hist:\n",
        "                break\n",
        "            new_hist.append(msg)\n",
        "            total += length\n",
        "        return list(reversed(new_hist))\n",
        "\n",
        "    def get_truncated_by_words(self, max_words: int) -> List[Message]:\n",
        "        total = 0\n",
        "        new_hist = []\n",
        "        for msg in reversed(self.history):\n",
        "            wc = len(msg.content.split())\n",
        "            if total + wc > max_words and new_hist:\n",
        "                break\n",
        "            new_hist.append(msg)\n",
        "            total += wc\n",
        "        return list(reversed(new_hist))\n",
        "\n",
        "\n",
        "    def get_serialized_history(self, use_truncated: List[Message] = None):\n",
        "        if use_truncated is not None:\n",
        "            return [m.serialize() for m in use_truncated]\n",
        "        return [m.serialize() for m in self.history]\n",
        "\n",
        "\n",
        "    def ask_model(self, use_truncated: List[Message] = None, model: str = None):\n",
        "\n",
        "        chosen_model = model or self.default_model  # use small model by default\n",
        "        response = client.chat.completions.create(\n",
        "            model=chosen_model,\n",
        "            messages=self.get_serialized_history(use_truncated)\n",
        "        )\n",
        "        reply = response.choices[0].message.content\n",
        "        self.add_message(\"assistant\", reply)\n",
        "        return reply\n",
        "\n",
        "    def print_history(self, history_to_print: List[Message] = None, show_timestamp=False):\n",
        "        hist = history_to_print if history_to_print is not None else self.history\n",
        "        print(\"\\n--- Conversation History ---\")\n",
        "        for m in hist:\n",
        "\n",
        "            ts = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") if show_timestamp else \"\"\n",
        "            ts_part = f\"{ts} | \" if show_timestamp else \"\"\n",
        "            print(f\"{ts_part}{m.role.upper()}: {m.content}\")\n",
        "        print(\"--- END ---\\n\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GOtzro8y1EJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv = ConversationHistory()\n",
        "\n",
        "samples = [\n",
        "    \"Hey, how are you?\",\n",
        "    \"Can you summarize a short story for me?\",\n",
        "    \"Once upon a time, there was a cat who wanted to fly.\",\n",
        "    \"Interesting! Can you tell me more about the cat's adventures?\",\n",
        "    \"Sure, the cat tried to build wings from leaves and sticks.\"\n",
        "]"
      ],
      "metadata": {
        "id": "S3pFzGpV1EPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for msg in samples:\n",
        "    conv.add_message(\"user\", msg)\n",
        "\n",
        "    conv.add_message(\"assistant\", f\"Echo: {msg}\")\n"
      ],
      "metadata": {
        "id": "o63VngC2uFqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo 1"
      ],
      "metadata": {
        "id": "wEJp8tr9Sd0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== LAST 3 TURNS ===\")\n",
        "trunc_turns = conv.get_truncated_by_turns(3)\n",
        "conv.print_history(trunc_turns)"
      ],
      "metadata": {
        "id": "6sU2CIsRuFuB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f471c05c-2915-4da9-80f8-2d69add9db01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== LAST 3 TURNS ===\n",
            "\n",
            "--- Conversation History ---\n",
            "ASSISTANT: Echo: Interesting! Can you tell me more about the cat's adventures?\n",
            "USER: Sure, the cat tried to build wings from leaves and sticks.\n",
            "ASSISTANT: Echo: Sure, the cat tried to build wings from leaves and sticks.\n",
            "--- END ---\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo 2"
      ],
      "metadata": {
        "id": "o4TIonPxSh7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trunc_turns = conv.get_truncated_by_turns(5)\n",
        "conv.print_history(trunc_turns)"
      ],
      "metadata": {
        "id": "1CCFZKHAuFxf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89fcf9df-74b1-4629-c692-53f2c2f9cb29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Conversation History ---\n",
            "ASSISTANT: Echo: Once upon a time, there was a cat who wanted to fly.\n",
            "USER: Interesting! Can you tell me more about the cat's adventures?\n",
            "ASSISTANT: Echo: Interesting! Can you tell me more about the cat's adventures?\n",
            "USER: Sure, the cat tried to build wings from leaves and sticks.\n",
            "ASSISTANT: Echo: Sure, the cat tried to build wings from leaves and sticks.\n",
            "--- END ---\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo 1"
      ],
      "metadata": {
        "id": "P34isypNSkj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== LAST 100 CHARACTERS ===\")\n",
        "trunc_chars = conv.get_truncated_by_chars(100)\n",
        "conv.print_history(trunc_chars)"
      ],
      "metadata": {
        "id": "hZyq4wTMuF1I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71f7cf7e-b2e8-45e5-e349-f5045af8567a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== LAST 100 CHARACTERS ===\n",
            "\n",
            "--- Conversation History ---\n",
            "ASSISTANT: Echo: Sure, the cat tried to build wings from leaves and sticks.\n",
            "--- END ---\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo 2"
      ],
      "metadata": {
        "id": "l5MRwebpSo7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== LAST 500 CHARACTERS ===\")\n",
        "trunc_chars = conv.get_truncated_by_chars(500)\n",
        "conv.print_history(trunc_chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hmiy9DF3ak18",
        "outputId": "44e9cfbc-da70-4445-c357-7bf907ce008b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== LAST 500 CHARACTERS ===\n",
            "\n",
            "--- Conversation History ---\n",
            "USER: Hey, how are you?\n",
            "ASSISTANT: Echo: Hey, how are you?\n",
            "USER: Can you summarize a short story for me?\n",
            "ASSISTANT: Echo: Can you summarize a short story for me?\n",
            "USER: Once upon a time, there was a cat who wanted to fly.\n",
            "ASSISTANT: Echo: Once upon a time, there was a cat who wanted to fly.\n",
            "USER: Interesting! Can you tell me more about the cat's adventures?\n",
            "ASSISTANT: Echo: Interesting! Can you tell me more about the cat's adventures?\n",
            "USER: Sure, the cat tried to build wings from leaves and sticks.\n",
            "ASSISTANT: Echo: Sure, the cat tried to build wings from leaves and sticks.\n",
            "--- END ---\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo 1\n"
      ],
      "metadata": {
        "id": "KO1-5MPvSsue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== LAST 20 WORDS ===\")\n",
        "trunc_words = conv.get_truncated_by_words(20)\n",
        "conv.print_history(trunc_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4S45-QuIak41",
        "outputId": "2828aaaf-7c02-415a-c44c-af6612d05ebd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== LAST 20 WORDS ===\n",
            "\n",
            "--- Conversation History ---\n",
            "ASSISTANT: Echo: Sure, the cat tried to build wings from leaves and sticks.\n",
            "--- END ---\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo 2"
      ],
      "metadata": {
        "id": "EpS87TDXSvSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== LAST 20 WORDS ===\")\n",
        "trunc_words = conv.get_truncated_by_words(50)\n",
        "conv.print_history(trunc_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8M9RSQtAak8P",
        "outputId": "3addf72c-d196-451f-8671-b4cc83ca0c45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== LAST 20 WORDS ===\n",
            "\n",
            "--- Conversation History ---\n",
            "USER: Interesting! Can you tell me more about the cat's adventures?\n",
            "ASSISTANT: Echo: Interesting! Can you tell me more about the cat's adventures?\n",
            "USER: Sure, the cat tried to build wings from leaves and sticks.\n",
            "ASSISTANT: Echo: Sure, the cat tried to build wings from leaves and sticks.\n",
            "--- END ---\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y-Yc4_LEak_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Add periodic summarization:"
      ],
      "metadata": {
        "id": "lQWfNh-ouRqz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "@dataclass\n",
        "class Message:\n",
        "    role: str\n",
        "    content: str\n",
        "\n",
        "    def serialize(self):\n",
        "        return {\"role\": self.role, \"content\": self.content}\n",
        "\n",
        "\n",
        "# ----- Conversation summaries and optional display -----\n",
        "class ConversationHistory:\n",
        "    def __init__(self, default_model=\"llama-3.1-8b-instant\", summarize_every_k=2, summary_window=2):\n",
        "        self.history: List[Message] = []\n",
        "        self.default_model = default_model\n",
        "        self.summarize_every_k = summarize_every_k\n",
        "        self.turn_counter = 0\n",
        "        self.summary_window = summary_window  # last N messages for summary\n",
        "        self.summary_history: List[str] = []  # track TL;DR summaries\n",
        "\n",
        "    # Add user message\n",
        "    def add_user(self, content: str):\n",
        "        self.history.append(Message(role=\"user\", content=content))\n",
        "\n",
        "    # Ask model and store reply\n",
        "    def ask_model(self, user_input: str):\n",
        "        self.add_user(user_input)\n",
        "\n",
        "        # Send conversation to API\n",
        "        response = client.chat.completions.create(\n",
        "            model=self.default_model,\n",
        "            messages=[m.serialize() for m in self.history]\n",
        "        )\n",
        "\n",
        "        reply = response.choices[0].message.content\n",
        "        self.history.append(Message(role=\"assistant\", content=reply))\n",
        "\n",
        "        # Increment turn and summarize if needed\n",
        "        self.turn_counter += 1\n",
        "        if self.turn_counter % self.summarize_every_k == 0:\n",
        "            self._summarize_history()\n",
        "\n",
        "        return reply\n",
        "\n",
        "    # 1-line TL;DR summary (safe for API)\n",
        "    def _summarize_history(self):\n",
        "        messages_to_summarize = self.history[-self.summary_window:]\n",
        "\n",
        "        summary_prompt = (\n",
        "            \"Summarize the following conversation in ONE super short sentence \"\n",
        "            \"(max 10 words), no extra details, only main point:\\n\\n\"\n",
        "        )\n",
        "        for msg in messages_to_summarize:\n",
        "            summary_prompt += f\"{msg.role}: {msg.content}\\n\"\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=self.default_model,\n",
        "            messages=[{\"role\": \"user\", \"content\": summary_prompt}]\n",
        "        )\n",
        "\n",
        "        summary_text = response.choices[0].message.content.strip()\n",
        "        summary_text = summary_text.replace(\"\\n\", \" \")\n",
        "        if '.' in summary_text:\n",
        "            summary_text = summary_text.split('.')[0]\n",
        "        summary_text = summary_text[:100]\n",
        "\n",
        "        # Store separately, not in API history\n",
        "        self.summary_history.append(summary_text)\n",
        "\n",
        "        print(\"\\n--- TL;DR Summary ---\")\n",
        "        print(summary_text)\n",
        "        print(\"--- END SUMMARY ---\\n\")\n",
        "\n",
        "    # Print conversation or summaries only\n",
        "    def print_history(self, summaries_only=False):\n",
        "        if summaries_only:\n",
        "            print(\"\\n--- TL;DR Summaries Only ---\")\n",
        "            for i, s in enumerate(self.summary_history, 1):\n",
        "                print(f\"Summary {i}: {s}\")\n",
        "            print(\"--- END ---\\n\")\n",
        "            return\n",
        "\n",
        "        print(\"\\n--- Full Conversation ---\")\n",
        "        for msg in self.history:\n",
        "            print(f\"{msg.role.upper()}: {msg.content}\")\n",
        "        if self.summary_history:\n",
        "            print(\"\\n--- Summaries ---\")\n",
        "            for i, s in enumerate(self.summary_history, 1):\n",
        "                print(f\"Summary {i}: {s}\")\n",
        "        print(\"--- END ---\\n\")\n"
      ],
      "metadata": {
        "id": "BOpd0KYMirYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo"
      ],
      "metadata": {
        "id": "2v0sfxjATa-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv = ConversationHistory(summarize_every_k=2)\n",
        "\n",
        "conv.ask_model(\"Hi, how are you?\")\n",
        "conv.ask_model(\"Can you tell me a short story about a cat?\")\n",
        "conv.ask_model(\"What did the cat do next?\")\n",
        "conv.ask_model(\"Make it funny!\")\n",
        "conv.ask_model(\"Add a twist at the end!\")\n",
        "\n",
        "# Only see summaries\n",
        "conv.print_history(summaries_only=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WP1IS8tQb0TV",
        "outputId": "61065ae5-4aac-44c8-f468-9c815238d594"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- TL;DR Summary ---\n",
            "A cat named Luna saves her elderly human\n",
            "--- END SUMMARY ---\n",
            "\n",
            "\n",
            "--- TL;DR Summary ---\n",
            "Luna becomes the village's unorthodox gardening expert\n",
            "--- END SUMMARY ---\n",
            "\n",
            "\n",
            "--- TL;DR Summaries Only ---\n",
            "Summary 1: A cat named Luna saves her elderly human\n",
            "Summary 2: Luna becomes the village's unorthodox gardening expert\n",
            "--- END ---\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv.history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTzOx5-Cb0WL",
        "outputId": "72c412c5-867e-4f70-b959-5b675a618912"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Message(role='user', content='Hi, how are you?'),\n",
              " Message(role='assistant', content=\"I'm functioning properly, thanks for asking. What can I help you with today?\"),\n",
              " Message(role='user', content='Can you tell me a short story about a cat?'),\n",
              " Message(role='assistant', content=\"Once upon a time, in a small village nestled between rolling hills, there lived a beautiful calico cat named Luna. Luna was a stray who had wandered into the village one day, seeking refuge and warmth. She quickly won over the hearts of the villagers, particularly an elderly woman named Elara, who welcomed Luna into her cozy cottage.\\n\\nElara took great care of Luna, feeding her the finest fish and brushing her luscious fur every day. As the days went by, Luna grew fonder of Elara, who spoke softly to her and shared her stories of adventure. The villagers marveled at the bond between Elara and Luna, who had formed an unbreakable connection.\\n\\nHowever, one winter night, a fierce storm rolled in, bringing heavy snow and powerful winds. Elara, now aging and frail, grew anxious and worried as the storm raged on. She called out to Luna, but the brave cat was nowhere to be found. As the villagers huddled together for warmth, they whispered that they had seen Luna slip out into the storm, her eyes fixed on a distant landmark.\\n\\nDays passed, and the villagers feared the worst for Luna, their beloved companion. But as the snow began to clear and the village slowly returned to normal, a miracle occurred. Luna, her coat caked with snow, her fur tangled and matted, but her spirit unbroken, reappeared at Elara's doorstep, mewling softly and purring as she rubbed against Elara's leg.\\n\\nOverjoyed to be reunited with her cherished companion, Elara wrapped Luna in a warm blanket and cared for her until the cat's fur regained its luster. From that day forward, the villagers revered Luna as a hero, and Elara and Luna's bond grew stronger than ever, a testament to the unbreakable tie between a kind human and her loyal, brave cat.\"),\n",
              " Message(role='user', content='What did the cat do next?'),\n",
              " Message(role='assistant', content=\"After her daring escapade during the storm, Luna continued to be a beloved and integral member of the village. She spent her days lounging in the sunbeams that streamed through Elara's windows, chasing the occasional mouse that dared to venture into the cottage, and napping on her favorite cushion.\\n\\nHowever, an unexpected event soon changed the course of Luna's life. Elara, who had grown too old to tend to the garden on her own, realized that she needed help to keep her beloved flowers blooming. She turned to Luna, who had always shown a keen interest in the garden.\\n\\nTo everyone's surprise, Luna proved to be a skilled gardener. She would spend hours patrolling the garden, chasing away rabbits and other pests that threatened to destroy Elara's carefully tended plants. As the days went by, Luna grew more confident, and under Elara's guidance, she began to learn the art of gardening.\\n\\nWith Luna by her side, the garden flourished. The cat would gently paw at the soil, planting seeds and watering the plants with her own special brand of care. The villagers marveled at the beauty of the garden, and Elara beamed with pride as she watched her cherished companion transform into a talented and vital member of the gardening community.\\n\\nWord of Luna's green thumb spread far and wide, and soon, villagers from neighboring towns came to seek her advice on how to tend their own gardens. Luna, ever the diplomat, would listen patiently to their concerns and offer her expert guidance, lapping up the attention and praise in her trademark feline fashion.\\n\\nAs she grew older and wiser, Luna became a respected figure in the village, known for her remarkable gardening skills, her fierce loyalty to Elara, and her unwavering spirit. And Elara, grateful for her companion's companionship, knew that she had found a true friend in this beautiful, talented, and brave calico cat named Luna.\"),\n",
              " Message(role='user', content='Make it funny!'),\n",
              " Message(role='assistant', content='After her daring escapade during the storm, Luna decided to take up a new career â€“ professional napping expert. She spent her days lounging in the sunbeams that streamed through Elara\\'s windows, chasing the occasional dust bunny that dared to venture into the cottage, and snoring loudly on her favorite cushion.\\n\\nHowever, when Elara\\'s gardening prowess began to wane with age, Luna stepped in to save the day. But, it quickly became apparent that her gardening skills were more of a \"wildcard\" affair. She would dig holes randomly, scattering seeds everywhere, and then claim that she was \"just making art.\"\\n\\nAs the garden descended into chaos, the villagers couldn\\'t help but laugh at the absurd sight of Luna, covered in mud, with a bouquet of wilting flowers clutched in her paw. Elara, chuckling along with them, exclaimed, \"Well, I guess that\\'s one way to get rid of pesky weeds!\"\\n\\nDetermined to become a respected gardener, Luna enrolled in the village\\'s \"Feline Horticultural Academy\" (FHA), but it didn\\'t take her long to realize that the courses were a bit...unconventional. The curriculum included \"Advanced Squirrel-Repellent Techniques\" and \"The Art of Napping Among Blooms,\" but skipped over the basics of, you know, actually growing plants.\\n\\nDespite this, Luna graduated at the top of her class (okay, she was the only cat in the class, but still!) and began offering her services as a \"Gardening Consultant\" to the villagers. Her advice ranged from \"Don\\'t worry, just add more glitter\" to \"If you\\'re having trouble with weeds, just give them a good belly rub â€“ they\\'ll wither away from embarrassment.\"\\n\\nNeedless to say, the villagers adored Luna and her unorthodox gardening methods. They\\'d often gather \\'round to watch her attempt to \"train\" her nemesis, a particularly clever squirrel named Nutmeg, by chasing him around the garden while wearing an invisible cape.\\n\\nAnd so, Luna remained the resident \"Gardening Cat\" of the village, spreading chaos and joy wherever she went. Elara would often smile, shaking her head in amusement, and say, \"That girl\\'s got more lives than a cat has lives\"...which, technically, is not true at all.'),\n",
              " Message(role='user', content='Add a twist at the end!'),\n",
              " Message(role='assistant', content='After her daring escapade during the storm, Luna decided to take up a new career â€“ professional napping expert. She spent her days lounging in the sunbeams that streamed through Elara\\'s windows, chasing the occasional dust bunny that dared to venture into the cottage, and snoring loudly on her favorite cushion.\\n\\nHowever, when Elara\\'s gardening prowess began to wane with age, Luna stepped in to save the day. But, it quickly became apparent that her gardening skills were more of a \"wildcard\" affair. She would dig holes randomly, scattering seeds everywhere, and then claim that she was \"just making art.\"\\n\\nAs the garden descended into chaos, the villagers couldn\\'t help but laugh at the absurd sight of Luna, covered in mud, with a bouquet of wilting flowers clutched in her paw. Elara, chuckling along with them, exclaimed, \"Well, I guess that\\'s one way to get rid of pesky weeds!\"\\n\\nDetermined to become a respected gardener, Luna enrolled in the village\\'s \"Feline Horticultural Academy\" (FHA), but it didn\\'t take her long to realize that the courses were a bit...unconventional. The curriculum included \"Advanced Squirrel-Repellent Techniques\" and \"The Art of Napping Among Blooms,\" but skipped over the basics of, you know, actually growing plants.\\n\\nDespite this, Luna graduated at the top of her class (okay, she was the only cat in the class, but still!) and began offering her services as a \"Gardening Consultant\" to the villagers. Her advice ranged from \"Don\\'t worry, just add more glitter\" to \"If you\\'re having trouble with weeds, just give them a good belly rub â€“ they\\'ll wither away from embarrassment.\"\\n\\nNeedless to say, the villagers adored Luna and her unorthodox gardening methods. They\\'d often gather \\'round to watch her attempt to \"train\" her nemesis, a particularly clever squirrel named Nutmeg, by chasing him around the garden while wearing an invisible cape.\\n\\nJust as Luna was basking in the glory of her newfound fame, a mysterious letter arrived at Elara\\'s doorstep. It was from a secret organization known as \"F.A.N.G.\" (Felines and Napping Gurus), and it revealed a shocking truth: Luna was not just any ordinary cat â€“ she was a highly trained undercover agent, tasked with infiltrating the village and creating chaos to distract from a sinister plot to steal all the world\\'s catnip.\\n\\nElara\\'s eyes widened as she read the letter, while Luna, still snoring peacefully on her cushion, seemed completely unaware of her true mission. As the villagers looked on in awe, Luna was suddenly whisked away by F.A.N.G.\\'s agents, leaving behind a trail of confusion, chaos, and a very confused Elara, who exclaimed, \"Wait, what about my garden?!\"')]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "@dataclass\n",
        "class Message:\n",
        "    role: str\n",
        "    content: str\n",
        "\n",
        "    def serialize(self):\n",
        "        # API only accepts 'user', 'assistant', 'system'\n",
        "        return {\"role\": \"system\" if self.role == \"summary\" else self.role, \"content\": self.content}\n",
        "\n",
        "\n",
        "class ConversationHistory:\n",
        "    def __init__(self, default_model=\"llama-3.1-8b-instant\", summarize_every_k=2, summary_window=3):\n",
        "        self.history: List[Message] = []\n",
        "        self.default_model = default_model\n",
        "        self.summarize_every_k = summarize_every_k\n",
        "        self.summary_window = summary_window\n",
        "        self.turn_counter = 0\n",
        "\n",
        "    # Add user message\n",
        "    def add_user(self, content: str):\n",
        "        self.history.append(Message(role=\"user\", content=content))\n",
        "\n",
        "    # Ask model\n",
        "    def ask_model(self, user_input: str):\n",
        "        self.add_user(user_input)\n",
        "\n",
        "        # Send current full history to API\n",
        "        response = client.chat.completions.create(\n",
        "            model=self.default_model,\n",
        "            messages=[m.serialize() for m in self.history]\n",
        "        )\n",
        "\n",
        "        reply = response.choices[0].message.content\n",
        "\n",
        "        # Add assistant message\n",
        "        self.history.append(Message(role=\"assistant\", content=reply))\n",
        "\n",
        "        # Increment turn counter and summarize if needed\n",
        "        self.turn_counter += 1\n",
        "        if self.turn_counter % self.summarize_every_k == 0:\n",
        "            self._summarize_history()\n",
        "\n",
        "        return reply\n",
        "\n",
        "    # Replace entire conversation history with 1-line TL;DR summary\n",
        "    def _summarize_history(self):\n",
        "        messages_to_summarize = self.history[-self.summary_window:]\n",
        "\n",
        "        prompt = \"Summarize this conversation in ONE short sentence (max 10 words):\\n\"\n",
        "        for m in messages_to_summarize:\n",
        "            prompt += f\"{m.role}: {m.content}\\n\"\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=self.default_model,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "        )\n",
        "\n",
        "        summary_text = response.choices[0].message.content.strip()\n",
        "        summary_text = summary_text.replace(\"\\n\", \" \")\n",
        "        if '.' in summary_text:\n",
        "            summary_text = summary_text.split('.')[0]\n",
        "\n",
        "        # REPLACE full history with just the summary\n",
        "        self.history = [Message(role=\"summary\", content=summary_text)]\n",
        "\n",
        "        print(summary_text)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fMpY1zT4m3Cp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo"
      ],
      "metadata": {
        "id": "lr2D7HP7TgGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv = ConversationHistory(summarize_every_k=2)\n",
        "\n",
        "conv.ask_model(\"Hi, how are you?\")\n",
        "conv.ask_model(\"Tell me a story about a cat.\")\n",
        "conv.ask_model(\"What happens next?\")\n",
        "conv.ask_model(\"Make it funny!\")\n",
        "\n",
        "conv.history\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "610LOeT4jGyG",
        "outputId": "4a263982-bf7a-4bf2-b398-3fea6013b60a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I was told a story about a friendship between a cat and a butterfly\n",
            "Cat and butterfly have hilarious late-night gossip session\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Message(role='summary', content='Cat and butterfly have hilarious late-night gossip session')]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bNtPhjWQb0cH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff004765-0c97-49ec-d3eb-11b3ab22a5e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'https://github.com/harshitabasrur/Developer-Internship-Assignment'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cd1TESmLd-9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4shjL7mJd_Ay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZTQx0ldUd_D1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ws2Zv1K4d_HO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}